# ğŸ¤– Context-Aware LangChain RAG Chatbot  
*A Modular Retrieval-Augmented Generation System built with LangChain, ChromaDB, and Multiple LLM Providers (OpenAI, Groq, Gemini)*  

[![LangChain](https://img.shields.io/badge/LangChain-0.3+-blue.svg)](https://www.langchain.com/)
[![ChromaDB](https://img.shields.io/badge/VectorDB-ChromaDB-purple.svg)](https://www.trychroma.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red.svg)](https://streamlit.io)
[![Python](https://img.shields.io/badge/Python-3.10+-yellow.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Groq API](https://img.shields.io/badge/Groq-LLM-orange.svg)](https://groq.com)
[![Gemini](https://img.shields.io/badge/Google-Gemini-blue.svg)](https://deepmind.google/technologies/gemini/)

---

## ğŸ§  Overview

This repository contains a **Context-Aware LangChain RAG (Retrieval-Augmented Generation) Chatbot**, a modular and production-ready system designed for **document-grounded question answering**.  

The chatbot loads and processes your local documents (PDF, TXT, MD), embeds them using **Sentence Transformers**, stores them in **ChromaDB**, and uses **LLMs** (Groq, OpenAI, or Gemini) to generate **accurate, non-hallucinated answers**.

Developed as part of the **Ready Tensor AI Developer Certification Program (2025)**, this project demonstrates advanced RAG pipelines, structured prompt-building, and LLM safety best practices.

---

## ğŸ¯ Problem it Solves

LLMs are often **not context-aware** and tend to **hallucinate** facts.  
This project solves that by integrating **retrieval-based context injection**, ensuring every answer is derived directly from trusted document sources.

---

## âš™ï¸ Key Features

âœ… Retrieval-Augmented Generation (RAG) pipeline  
âœ… Multi-LLM support (Groq, OpenAI, Gemini)  
âœ… Structured prompt generation for consistency  
âœ… Vector search with ChromaDB and Sentence Transformers  
âœ… Document-grounded, anti-hallucination responses  
âœ… Streamlit chat UI for real-time interaction  
âœ… Prompt-injection and context leakage defense  

---

## ğŸ§© Project Structure

project/
    |___src/
    |   |__data/                      # Folder for user documents (PDF/TXT/MD)
    |   |   |__chatbotdoc.pdf         # Dependencies list
    |   â”‚
    |   â”œâ”€â”€ app.py                    # Core RAG logic and orchestration
    |   â”œâ”€â”€ vectordb.py               # Embedding generation + ChromaDB store
    |   â”œâ”€â”€ prompt_builder.py         # Builds structured prompts dynamically
    |   â”œâ”€â”€ prompt_instructions.py    # System rules, tone, and safety constraints
    |   â”œâ”€â”€ run.py                    # Streamlit UI for interactive chatting
    |               
    |   
    |â”€â”€.env                           # Environment variables (API keys, paths)
    |__.gitignore
    |__LICENSE
    |__README.md
    |__requirements.txt                

---

## ğŸ§± System Architecture

```mermaid
flowchart TD
    A[User Query] --> B[VectorDB Search (Chroma)]
    B --> C[Retrieve Top-k Relevant Chunks]
    C --> D[PromptBuilder Combines Context + Query]
    D --> E[LLM (Groq/OpenAI/Gemini) Generates Response]
    E --> F[Streamlit UI Displays Answer]
```

---

## âš™ï¸ How It Works

### Load Documents
- Reads PDFs or text files from `/data`.
- Splits text into semantic chunks with LangChainâ€™s text splitters.

### Embed & Store
- Converts text chunks into vector embeddings using `sentence-transformers/all-MiniLM-L6-v2`.
- Stores embeddings in **ChromaDB**.

### Retrieve Context
- User queries trigger a similarity search to fetch the most relevant document chunks.

### Build Prompt
- **PromptBuilder** composes a structured system prompt using rules from **PromptInstructions**.
- Defines tone, safety, and reasoning constraints.

### Generate Answer
- The LLM (Groq, OpenAI, or Gemini) processes the context + user query.
- Produces document-grounded answers only.

### Display in Streamlit
- Chat-style UI for seamless user interaction.

---

## ğŸ” Safety & Integrity Features

ğŸ›¡ï¸ **Instruction Shielding** â€“ Blocks prompt-injection & reasoning exposure  
ğŸ“˜ **Context Restriction** â€“ Answers strictly from retrieved document content  
ğŸš« **No Hallucination** â€“ Declines politely when info is unavailable  
ğŸ’¬ **Response Control** â€“ Maintains word count between 40â€“160  

Example fallback response:
> â€œIâ€™m sorry, it seems that information isnâ€™t covered in the document provided. Could you specify a topic or section to explore?â€

---

## ğŸŒ Supported LLM Providers

| Provider | Package | Env Variables |
|-----------|----------|----------------|
| **Groq** | langchain-groq | `GROQ_API_KEY`, `GROQ_MODEL` |
| **OpenAI** | langchain-openai | `OPENAI_API_KEY`, `OPENAI_MODEL` |
| **Google Gemini** | langchain-google-genai | `GOOGLE_API_KEY`, `GOOGLE_MODEL` |

---

## ğŸ§© Installation

```bash
# Clone the repository
git clone https://github.com/Richmondiroegbu/context-aware-rag-assistant.git
cd <src>

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate   # (Windows: venv\Scripts\activate)

# Install dependencies
pip install -r requirements.txt
```

Create a `.env` file in the project root:

```ini
OPENAI_API_KEY=your_openai_key_here
GROQ_API_KEY=your_groq_key_here
GOOGLE_API_KEY=your_google_key_here
DATA_PATH=./data
```

Add your documents (e.g., *Agenda 2063: The Africa We Want â€“ Popular Version.pdf*) into the `/data` folder.

---

## â–¶ï¸ Running the App

Run the Streamlit interface:

```bash
streamlit run run.py
```

Then open your browser to:  
ğŸ‘‰ **http://localhost:8501**
   **http://192.168.0.187:8501**
    

---

## ğŸ’¬ Example Chat

**User:**  
> What are the main goals of Agenda 2063?

**Assistant:**  
> According to the document, Agenda 2063 envisions a prosperous and unified Africa driven by inclusive growth, sustainable development, and regional integration, promoting peace and global influence.

---


## ğŸ§© Future Enhancements

ğŸ§  Add memory for multi-turn contextual chats  
ğŸŒ Real-time WebSocket streaming for faster response  
ğŸ“¤ Upload documents directly via Streamlit  
ğŸ§® Add retrieval evaluation metrics (precision / recall)  
ğŸ’» Integrate local LLMs (Ollama, LM Studio) for offline inference  

---

## ğŸ§‘â€ğŸ’» Author

**Richmond Iroegbu**  
*Data Science & Machine Learning Engineer*  
ğŸ“œ Developed for the **Ready Tensor AI Developer Certification Program (2025)**  
ğŸ”— *LinkedIn Â· GitHub*

---

## ğŸªª License

This project is licensed under the **MIT License** â€” free for personal and commercial use.  
See the [LICENSE](LICENSE) file for details.

---

## ğŸŒŸ Acknowledgments

- **LangChain** â€“ Modular framework for LLM apps  
- **ChromaDB** â€“ Vector store and retrieval engine  
- **Sentence Transformers** â€“ For text embeddings  
- **Streamlit** â€“ Frontend interface  
- **Groq API** â€“ High-performance inference provider  
- **Google Gemini** â€“ Multi-modal LLM  

ğŸ’¡ This repository demonstrates a reliable, ethical, and scalable LangChain-based RAG architecture designed for real-world AI applications.

